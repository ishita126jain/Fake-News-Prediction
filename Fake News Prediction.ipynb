{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85ce995",
   "metadata": {},
   "source": [
    "# About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3291d",
   "metadata": {},
   "source": [
    "1.id: unique id for a news article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ac2ed",
   "metadata": {},
   "source": [
    "2.title: the title of a news article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f13bf",
   "metadata": {},
   "source": [
    "3.author: author of the news article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4db30",
   "metadata": {},
   "source": [
    "4.text: the text of the article, could be incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022163a",
   "metadata": {},
   "source": [
    "5.label: a label that marks whether the news article is real or fake, if 1 fake news else if 0 real news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a9783",
   "metadata": {},
   "source": [
    "# Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7b655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # re for regular expression it is very useful for searching the text in the document.\n",
    "from nltk.corpus import stopwords #nltk: natural languaue tool kit and corpus meanse the body of the particular text or important content of the text. Stopwords are those words which doesn't matter or important in the text.\n",
    "from nltk.stem.porter import PorterStemmer #We perform the process called stemming. Stemming is the process in which we take a word and remove the prefix and suffix of the word and returns the root word of it.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #used to convert text into feature vectors.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd76841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ishita/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #Download a stopword from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55edc1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Lets print downloaded stopwords of english.\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44634a64",
   "metadata": {},
   "source": [
    "# Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874cce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset to a pandas Dataframe\n",
    "news_dataset = pd.read_csv('/home/ishita/Documents/datasets/fake_news_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d9eae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae19490a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print first five rows fo the dataset\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aad5621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of missing values in dataset\n",
    "news_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa345b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have large dataset so we can replace the missing values by empty string\n",
    "news_dataset = news_dataset.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b2c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb481d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining title and author name in new feature called content\n",
    "news_dataset['content']= news_dataset['author']+' '+news_dataset['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472e75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Darrell Lucus House Dem Aide: We Didn’t Even S...\n",
      "1        Daniel J. Flynn FLYNN: Hillary Clinton, Big Wo...\n",
      "2        Consortiumnews.com Why the Truth Might Get You...\n",
      "3        Jessica Purkiss 15 Civilians Killed In Single ...\n",
      "4        Howard Portnoy Iranian woman jailed for fictio...\n",
      "                               ...                        \n",
      "20795    Jerome Hudson Rapper T.I.: Trump a ’Poster Chi...\n",
      "20796    Benjamin Hoffman N.F.L. Playoffs: Schedule, Ma...\n",
      "20797    Michael J. de la Merced and Rachel Abrams Macy...\n",
      "20798    Alex Ansary NATO, Russia To Hold Parallel Exer...\n",
      "20799              David Swanson What Keeps the F-35 Alive\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news_dataset['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8c1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the content feature and labels for prediction.\n",
    "#seperating the data and label\n",
    "X = news_dataset.drop(columns='label',axis=1)\n",
    "Y = news_dataset['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685f103",
   "metadata": {},
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48c381",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4081677e",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its root word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf612a",
   "metadata": {},
   "source": [
    "for example root word for actor, actress, acting is act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f140ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0036a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content) #remove all the numbers or puntuation sign from the the content and replace it with space as shown later in place of older content we replace from new content, here re is regular expression and sub is substitution.\n",
    "    stemmed_content = stemmed_content.lower()  #convert all words to lowercase letter\n",
    "    stemmed_content = stemmed_content.split() #all these sentences are splited and converted into list.\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')] #take each word and convert it into root word and also remove all the stopwords\n",
    "    stemmed_content = ' '.join(stemmed_content) #joining all words and we have a processed content.\n",
    "    return stemmed_content #finally we return the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "710f322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset['content'] = news_dataset['content'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "358a28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        darrel lucu hous dem aid even see comey letter...\n",
      "1        daniel j flynn flynn hillari clinton big woman...\n",
      "2                   consortiumnew com truth might get fire\n",
      "3        jessica purkiss civilian kill singl us airstri...\n",
      "4        howard portnoy iranian woman jail fiction unpu...\n",
      "                               ...                        \n",
      "20795    jerom hudson rapper trump poster child white s...\n",
      "20796    benjamin hoffman n f l playoff schedul matchup...\n",
      "20797    michael j de la merc rachel abram maci said re...\n",
      "20798    alex ansari nato russia hold parallel exercis ...\n",
      "20799                            david swanson keep f aliv\n",
      "Name: content, Length: 20800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(news_dataset['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7076df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the data and label\n",
    "X = news_dataset['content'].values\n",
    "Y = news_dataset['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c318d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['darrel lucu hous dem aid even see comey letter jason chaffetz tweet'\n",
      " 'daniel j flynn flynn hillari clinton big woman campu breitbart'\n",
      " 'consortiumnew com truth might get fire' ...\n",
      " 'michael j de la merc rachel abram maci said receiv takeov approach hudson bay new york time'\n",
      " 'alex ansari nato russia hold parallel exercis balkan'\n",
      " 'david swanson keep f aliv']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abf9ae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce7bf864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8de79c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the content is all text value and computer cannot able to understand the texual language so we need to convert it into meaningful numbers (feature vector).\n",
    "#converting the texual data to numerical data\n",
    "vectorizer = TfidfVectorizer() # Tf stands for term frequency (number of time that word is reapeting in the text or paragraph) and idf stands for inverse document frequency (some time the word repeate several times dosen't have meaning in it for example in reviewof any movie the movie name is repeated several times which donot have any meaning\n",
    "vectorizer.fit(X)\n",
    "\n",
    "X = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8de88010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15686)\t0.28485063562728646\n",
      "  (0, 13473)\t0.2565896679337957\n",
      "  (0, 8909)\t0.3635963806326075\n",
      "  (0, 8630)\t0.29212514087043684\n",
      "  (0, 7692)\t0.24785219520671603\n",
      "  (0, 7005)\t0.21874169089359144\n",
      "  (0, 4973)\t0.233316966909351\n",
      "  (0, 3792)\t0.2705332480845492\n",
      "  (0, 3600)\t0.3598939188262559\n",
      "  (0, 2959)\t0.2468450128533713\n",
      "  (0, 2483)\t0.3676519686797209\n",
      "  (0, 267)\t0.27010124977708766\n",
      "  (1, 16799)\t0.30071745655510157\n",
      "  (1, 6816)\t0.1904660198296849\n",
      "  (1, 5503)\t0.7143299355715573\n",
      "  (1, 3568)\t0.26373768806048464\n",
      "  (1, 2813)\t0.19094574062359204\n",
      "  (1, 2223)\t0.3827320386859759\n",
      "  (1, 1894)\t0.15521974226349364\n",
      "  (1, 1497)\t0.2939891562094648\n",
      "  (2, 15611)\t0.41544962664721613\n",
      "  (2, 9620)\t0.49351492943649944\n",
      "  (2, 5968)\t0.3474613386728292\n",
      "  (2, 5389)\t0.3866530551182615\n",
      "  (2, 3103)\t0.46097489583229645\n",
      "  :\t:\n",
      "  (20797, 13122)\t0.2482526352197606\n",
      "  (20797, 12344)\t0.27263457663336677\n",
      "  (20797, 12138)\t0.24778257724396507\n",
      "  (20797, 10306)\t0.08038079000566466\n",
      "  (20797, 9588)\t0.174553480255222\n",
      "  (20797, 9518)\t0.2954204003420313\n",
      "  (20797, 8988)\t0.36160868928090795\n",
      "  (20797, 8364)\t0.22322585870464118\n",
      "  (20797, 7042)\t0.21799048897828688\n",
      "  (20797, 3643)\t0.21155500613623743\n",
      "  (20797, 1287)\t0.33538056804139865\n",
      "  (20797, 699)\t0.30685846079762347\n",
      "  (20797, 43)\t0.29710241860700626\n",
      "  (20798, 13046)\t0.22363267488270608\n",
      "  (20798, 11052)\t0.4460515589182236\n",
      "  (20798, 10177)\t0.3192496370187028\n",
      "  (20798, 6889)\t0.32496285694299426\n",
      "  (20798, 5032)\t0.4083701450239529\n",
      "  (20798, 1125)\t0.4460515589182236\n",
      "  (20798, 588)\t0.3112141524638974\n",
      "  (20798, 350)\t0.28446937819072576\n",
      "  (20799, 14852)\t0.5677577267055112\n",
      "  (20799, 8036)\t0.45983893273780013\n",
      "  (20799, 3623)\t0.37927626273066584\n",
      "  (20799, 377)\t0.5677577267055112\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734cd15",
   "metadata": {},
   "source": [
    "# Splitting the dataset to training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e200d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7047e",
   "metadata": {},
   "source": [
    "# Training the Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c5b8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74b53c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train) #train using training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009414d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae649dcf",
   "metadata": {},
   "source": [
    "accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "491f7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on the training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbfeab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the test data:  0.9865985576923076\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the training data: ',training_data_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1602447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score on the training data\n",
    "X_test_prediction = model.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02615405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the test data:  0.9790865384615385\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the test data: ',test_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a27b6c2",
   "metadata": {},
   "source": [
    "# Making a Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ceff9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "The news is Real\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[3]\n",
    "\n",
    "prediction = model.predict(X_new)\n",
    "print(prediction)\n",
    "\n",
    "if (prediction[0]==0):\n",
    "    print('The news is Real')\n",
    "else:\n",
    "    print('The news is Fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9c28f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
